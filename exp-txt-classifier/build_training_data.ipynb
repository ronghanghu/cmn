{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_file = './data/vocab.txt'\n",
    "\n",
    "ann_file = './data/image_ann_dict.npz'\n",
    "explanation_feat_dir = './data/explanation_feat/'\n",
    "save_dir = './data/training_batches/'\n",
    "\n",
    "T = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ann_dict = np.load(ann_file)['im_name2ann'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find the maximum number of noun phrases in the expression (used to determine the T in LSTM)\n",
    "max_noun_phrase_num = 0\n",
    "for _, ann in ann_dict.items():\n",
    "    for explanation_info in ann:\n",
    "        max_noun_phrase_num = max(max_noun_phrase_num, len(explanation_info['noun_phrases']))\n",
    "\n",
    "print('maximum number of noun phrases:', max_noun_phrase_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(vocab_file) as f:\n",
    "    vocab_list = [l.lower().strip() for l in f.readlines()]\n",
    "num_vocab = len(vocab_list)\n",
    "word2vocab_idx = {v: n_v for n_v, v in enumerate(vocab_list)}\n",
    "print('number of words in vocab_list:', num_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "SENTENCE_SPLIT_REGEX = re.compile(r'(\\W+)')\n",
    "def tokenize(sentence):\n",
    "    tokens = SENTENCE_SPLIT_REGEX.split(sentence.lower())\n",
    "    tokens = [t.strip() for t in tokens if len(t.strip()) > 0]\n",
    "    return tokens\n",
    "\n",
    "def noun_phrases_to_bow(phrase, vocab_list):\n",
    "    '''\n",
    "    Construct bag-of-words features from a phrase\n",
    "    '''\n",
    "    bow = np.zeros(len(vocab_list), np.float32)\n",
    "    words = tokenize(phrase)\n",
    "    for w in words:\n",
    "        if w in word2vocab_idx:\n",
    "            bow[word2vocab_idx[w]] += 1\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for im_name, ann in ann_dict.items():\n",
    "    N = len(ann)\n",
    "    \n",
    "    seq_length_batch = np.zeros(N, np.int32)\n",
    "    label_batch = np.zeros(N, np.bool)\n",
    "    \n",
    "    bow_batch = np.zeros((T, N, num_vocab), np.float32)\n",
    "    visfeat_batch = np.zeros((T, N, 4096 + 8), np.float32)\n",
    "    bbox_score_batch = np.zeros((T, N, 1), np.float32)\n",
    "    \n",
    "    explanation_feat_file = os.path.join(explanation_feat_dir, im_name.replace('.jpg', '.npz'))\n",
    "    explanation_feat = np.load(explanation_feat_file)['explanation_feat'][()]\n",
    "    \n",
    "    query_dict = explanation_feat['query_dict']\n",
    "    for n, explanation_info in enumerate(ann):\n",
    "        label_batch[n] = explanation_info['label']\n",
    "        seq_length_batch[n] = 1 + len(explanation_info['noun_phrases'])\n",
    "        \n",
    "        # Put the whole sentence feature at the beginning\n",
    "        bow_batch[0, n] = noun_phrases_to_bow(explanation_info['explanation'], vocab_list)\n",
    "        visfeat_batch[0, n]  = explanation_feat['im_visfeat']\n",
    "        bbox_score_batch[0, n] = 0  # assign 0 as scores of every sentence\n",
    "        \n",
    "        for t, p in enumerate(explanation_info['noun_phrases']):\n",
    "            phrase = p['phrase']\n",
    "            bow_batch[t+1, n] = noun_phrases_to_bow(phrase, vocab_list)\n",
    "            visfeat_batch[t+1, n] = query_dict[phrase]['visfeat']\n",
    "            bbox_score_batch[t+1, n] = query_dict[phrase]['score']\n",
    "    \n",
    "    save_file = os.path.join(save_dir, im_name.replace('.jpg', '.npz'))\n",
    "    os.makedirs(os.path.dirname(save_file), exist_ok=True)\n",
    "    \n",
    "    np.savez(save_file,\n",
    "             seq_length_batch=seq_length_batch,\n",
    "             label_batch=label_batch,\n",
    "             bow_batch=bow_batch,\n",
    "             visfeat_batch=visfeat_batch,\n",
    "             bbox_score_batch=bbox_score_batch)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
